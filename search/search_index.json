{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to daidai","text":"daidai \ud83c\udf4a <p> Modern dependency &amp; assets management library for MLOps </p> <p> </p> <p>daidai \ud83c\udf4a is a minimalist, type-safe dependency management system for AI/ML components that streamlines workflow development with dependency injection, intelligent caching and seamless file handling.</p> <p>\ud83d\udea7 daidai is still very much a work in progress and is definitely not prod-ready. It is currently developed as a selfish software to become my personal go-to MLOps library, but feel free to give it a try :) \ud83d\udea7</p>"},{"location":"#why-daidai","title":"Why daidai?","text":"<p>Built for both rapid prototyping and production ML workflows, daidai \ud83c\udf4a:</p> <ul> <li>\ud83d\ude80 Accelerates Development - Reduces iteration cycles with zero-config caching</li> <li>\ud83e\udde9 Simplifies Architecture - Define reusable components with clear dependencies</li> <li>\ud83d\udd0c Works Anywhere - Seamless integration with cloud/local storage via fsspec (local, s3, gcs, az, ftp, hf..)</li> <li>\ud83e\udde0 Stays Out of Your Way - Type-hint based DI means minimal boilerplate</li> <li>\ud83e\uddf9 Manages Resources - Automatic cleanup prevents leaks and wasted compute</li> <li>\ud83e\uddea Enables Testing - Inject mock dependencies / stubs with ease for robust unit testing</li> <li>\ud83e\udeb6 Requires Zero Dependencies - Zero-dependency Core philosophy, install optionals at will</li> <li>\u03bb Promotes Functional Thinking - Embraces pure functions, immutability, and composition for predictable workflows</li> <li>\ud83e\uddf0 Adapts to Your Design - pure functions enable seamless integration with your preferred caching, versioning, validation systems..</li> </ul> <p>daidai is named after the Japanese word for \"orange\" \ud83c\udf4a, a fruit that is both sweet and sour, just like the experience of managing dependencies in ML projects. It is being developed with user happiness in mind, while providing great flexibility and minimal boilerplate. It has been inspired by pytest, modelkit, dependency injection &amp; testing principles and functional programming.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code># Full installation with all features\npip install daidai[all]\n\n# Core functionality of artifacts &amp; predictors\npip install daidai\n\n# With file handling support, disk caching, and fsspec\npip install daidai[files]\n\n# With memory usage tracking\npip install daidai[memory]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import base64\nfrom typing import Annotated, Any\n\nimport openai\n\nfrom daidai import ModelManager, artifact, predictor\n\n# Define artifacts which are long-lived objects\n# that can be used by multiple predictors, or other artifacts\n\n\n@artifact\ndef openai_client(**configuration: dict[str, Any]) -&gt; openai.OpenAI:\n    return openai.OpenAI(**configuration)\n\n\n# Fetch a distant file from HTTPS, but it can be from any source: local, S3, GCS, Azure, FTP, HF Hub, etc.\n@artifact\ndef dogo_picture(\n    picture: Annotated[\n        bytes,\n        \"https://images.pexels.com/photos/220938/pexels-photo-220938.jpeg\",\n        {\"cache_strategy\": \"no_cache\"},\n    ],\n) -&gt; str:\n    return base64.b64encode(picture).decode(\"utf-8\")\n\n\n# Define a predictor that depends on the previous artifacts\n# which are automatically loaded and passed as an argument\n\n\n@predictor\ndef ask(\n    message: str,\n    dogo_picture: Annotated[str, dogo_picture],\n    client: Annotated[openai.OpenAI, openai_client, {\"timeout\": 5}],\n    model: str = \"gpt-4o-mini\",\n) -&gt; str:\n    response = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": message},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{dogo_picture}\",\n                            \"detail\": \"low\",\n                        },\n                    },\n                ],\n            }\n        ],\n        model=model,\n    )\n    return response.choices[0].message.content\n\n\n# daidai takes care of loading dependencies &amp; injecting artifacts!\nprint(ask(\"Hello, what's in the picture ?\"))\n# &gt;&gt;&gt; The picture features a dog with a black and white coat.\n\n# Or manage lifecycle with context manager for production usage\n# all predictors, artifacts and files are automatically loaded and cleaned up\nwith ModelManager(preload=[ask]):\n    print(ask(\"Hello, what's in the picture ?\"))\n\n# or manually pass dependencies\nmy_other_openai_client = openai.OpenAI(timeout=0.1)\nprint(ask(\"Hello, what's in the picture ?\", client=my_other_openai_client))\n# &gt;&gt;&gt; openai.APITimeoutError: Request timed out.\n# OOOPS, the new client timed out, of course :-)\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Clean things up now that the UX has landed</li> <li> Protect file operations for parallelism / concurrency</li> <li> Add docs</li> <li> Add tests (unit, integration, e2e)</li> <li> Add a cookbook with common patterns &amp; recipes</li> <li> Add support for async components</li> <li> Enjoy the fruits of my labor \ud83c\udf4a</li> </ul>"},{"location":"#core-concepts","title":"\ud83e\udde0 Core Concepts","text":"<p><code>daidai</code> is built around a few key concepts that work together to provide a streamlined experience for developing and deploying ML components. The following explains these core concepts and how they interact.</p> <p>At the heart of <code>daidai</code> are two types of components: Artifacts and Predictors.</p>"},{"location":"#artifacts","title":"\ud83e\udde9 Artifacts","text":"<p>Artifacts represent long-lived objects that are typically expensive to create and should be reused across multiple operations, e.g.: Loaded ML models (or parts of: weights etc.), Embedding models, Customer Configurations, Tokenizers, Database connections, API clients..</p> <p>Artifacts have several important characteristics:</p> <ol> <li>They are computed once and cached, making them efficient for repeated use</li> <li>They can depend on other artifacts or files</li> <li>They are automatically cleaned up when no longer needed</li> <li>They can implement resource cleanup through generator functions</li> </ol> <p>Artifacts are defined using the <code>@artifact</code> decorator:</p> <pre><code>@artifact\ndef bert_model(\n    model_path: Annotated[Path, \"s3://models/bert-base.pt\"]\n) -&gt; BertModel:\n    return BertModel.from_pretrained(model_path)\n</code></pre>"},{"location":"#predictors","title":"\ud83d\udd2e Predictors","text":"<p>Predictors are functions that use artifacts to perform actual computations or predictions. Unlike artifacts:</p> <ol> <li>They are not cached themselves</li> <li>They are meant to be called repeatedly with different inputs</li> <li>They can depend on multiple artifacts or even other predictors</li> <li>They focus on the business logic of your application</li> </ol> <p>Predictors are defined using the <code>@predictor</code> decorator:</p> <pre><code>@predictor\ndef classify_text(\n    text: str,\n    model: Annotated[BertModel, bert_model],\n    tokenizer: Annotated[Tokenizer, tokenizer]\n) -&gt; str:\n    tokens = tokenizer(text)\n    prediction = model(tokens)\n    return prediction.label\n</code></pre>"},{"location":"#dependency-injection","title":"\ud83d\udc89 Dependency Injection","text":"<p><code>daidai</code> uses a type-hint based dependency injection system that minimizes boilerplate while providing type safety. The system works as follows:</p>"},{"location":"#type-annotations","title":"Type Annotations","text":"<p>Dependencies are declared using Python's <code>Annotated</code> type from the <code>typing</code> module:</p> <pre><code>param_name: Annotated[Type, Dependency, Optional[Configuration]]\n</code></pre> <p>Where:</p> <ul> <li><code>Type</code> is the expected type of the parameter</li> <li><code>Dependency</code> is the function that will be called to obtain the dependency</li> <li><code>Optional[Configuration]</code> is an optional dictionary of configuration parameters</li> </ul>"},{"location":"#automatic-resolution","title":"Automatic Resolution","text":"<p>When you call a predictor or artifact, <code>daidai</code> automatically:</p> <ol> <li>Identifies all dependencies (predictors, artifacts and files)</li> <li>Resolves the dependency graph</li> <li>Loads or retrieves cached dependencies</li> <li>Injects them into your function</li> </ol> <p>This happens transparently, so you can focus on your business logic rather than dependency management.</p> Simple Dependency Resolution Flowchart <p>For a single predictor with one artifact dependency having one file dependency, the dependency resolution flow looks like this:</p> <pre><code>flowchart TD\n    A[User calls Predictor] --&gt; B{Predictor in cache?}\n\n    subgraph \"Dependency Resolution\"\n        B --&gt;|No| D[Resolve Dependencies]\n\n        subgraph \"Artifact Resolution\"\n            D --&gt; E{Artifact in cache?}\n            E --&gt;|No| G[Resolve Artifact Dependencies]\n\n            subgraph \"File Handling\"\n                G --&gt; H{File in cache?}\n                H --&gt;|No| J[Download File]\n                J --&gt; K[Cache File based on Strategy]\n                K --&gt; I[Get Cached File]\n                H --&gt;|Yes| I\n            end\n\n            I --&gt; L[Deserialize File to Required Format]\n            L --&gt; M[Compute Artifact with File]\n            M --&gt; N[Cache Artifact]\n            N --&gt; F[Get Cached Artifact]\n            E --&gt;|Yes| F\n        end\n\n        F --&gt; O[Create Predictor Partial Function]\n        O --&gt; P[Cache Prepared Predictor]\n    end\n\n    B --&gt;|Yes| C[Get Cached Predictor]\n    P --&gt; C\n\n    subgraph \"Execution\"\n        C --&gt; Q[Execute Predictor Function]\n    end\n\n    Q --&gt; R[Return Result to User]\n</code></pre>"},{"location":"#manual-overrides","title":"Manual Overrides","text":"<p>You can always override automatic dependency injection by explicitly passing values:</p> <pre><code># Normal automatic injection\nresult = classify_text(\"Sample text\")\n\n# Override with custom model\ncustom_model = load_my_custom_model()\nresult = classify_text(\"Sample text\", model=custom_model)\n</code></pre> <p>This way, you can easily swap out components for testing, debugging, or A/B testing.</p>"},{"location":"#file-dependencies","title":"\ud83d\udce6 File Dependencies","text":"<p><code>daidai</code> provides first-class support for file dependencies through the same annotation system, through the <code>files</code> extra: <code>pip install daidai[files]</code></p> <pre><code>@artifact\ndef word_embeddings(\n    embeddings_file: Annotated[\n        Path,\n        \"s3://bucket/glove.txt\",\n        {\"cache_strategy\": \"on_disk\"}\n    ]\n) -&gt; Dict[str, np.ndarray]:\n    # Load, process and return embeddings as a dict\n</code></pre>"},{"location":"#file-types","title":"File Types","text":"<p><code>daidai</code> supports various file types through type hints:</p> <ul> <li><code>Path</code>: Returns a Path object pointing to the downloaded file</li> <li><code>str</code>: Returns the file content as a string</li> <li><code>bytes</code>: Returns the file content as bytes</li> <li><code>TextIO</code>: Returns a text file handle (similar to <code>open(file, \"r\")</code>)</li> <li><code>BinaryIO</code>: Returns a binary file handle (similar to <code>open(file, \"rb\")</code>)</li> <li><code>Generator[str]</code>: Returns a generator that yields lines from the file</li> <li><code>Generator[bytes]</code>: Returns a generator that yields chunks of binary data</li> </ul>"},{"location":"#cache-strategies","title":"Cache Strategies","text":"<p><code>daidai</code> offers multiple caching strategies for files:</p> <ul> <li><code>on_disk</code>: Download once and keep permanently in the cache directory</li> <li><code>on_disk_temporary</code>: Download to a temporary location, deleted when the process exits</li> <li><code>no_cache</code>: Do not cache the file, fetch it each time. Useful for dynamic content, or when you do not have permission to write to disk</li> </ul>"},{"location":"#storage-systems","title":"Storage Systems","text":"<p>Thanks to <code>fsspec</code> integration, <code>daidai</code> supports a wide range of storage systems:</p> <ul> <li>Local file system</li> <li>Amazon S3</li> <li>Google Cloud Storage</li> <li>Microsoft Azure Blob Storage</li> <li>SFTP/FTP</li> <li>HTTP/HTTPS</li> <li>Hugging Face Hub</li> <li>And many more through fsspec protocols</li> </ul>"},{"location":"#resource-lifecycle-management","title":"\ud83e\uddf9 Resource Lifecycle Management","text":"<p><code>daidai</code> automatically manages the lifecycle of resources to prevent leaks and ensure clean shutdown:</p>"},{"location":"#automatic-cleanup","title":"Automatic Cleanup","text":"<p>For basic resources, artifacts are automatically released when they're no longer needed. For resources requiring explicit cleanup (like database connections), <code>daidai</code> supports generator-based cleanup:</p> <pre><code>@artifact\ndef database_connection(db_url: str):\n    # Establish connection\n    conn = create_connection(db_url)\n    try:\n        yield conn  # Return the connection for use\n    finally:\n        conn.close()  # This runs during cleanup\n</code></pre>"},{"location":"#modelmanager","title":"ModelManager","text":"<p>The <code>ModelManager</code> class provides explicit control over component lifecycle and is the recommended way for production usage:</p> <pre><code># Preload components and manage their lifecycle\nwith ModelManager(preload=[classify_text]) as manager:\n    # Components are ready to use\n    result = classify_text(\"Sample input\")\n    # More operations...\n# All resources are automatically cleaned up\n</code></pre> <p>ModelManager features:</p> <ul> <li>Preloading of components for predictable startup times</li> <li>Namespace isolation for managing different environments</li> <li>Explicit cleanup on exit</li> <li>Support for custom configuration</li> </ul>"},{"location":"#namespaces","title":"Namespaces","text":"<p><code>daidai</code> supports isolating components into namespaces, which is useful for:</p> <ul> <li>Running multiple model versions concurrently</li> <li>Testing with different configurations</li> <li>Implementing A/B testing</li> </ul> <pre><code># Production namespace\nwith ModelManager(preload=[model_v1], namespace=\"prod\"):\n    # Development namespace in the same process\n    with ModelManager(preload=[model_v2], namespace=\"dev\"):\n        # Both can be used without conflicts\n        prod_result = model_v1(\"input\")\n        dev_result = model_v2(\"input\")\n</code></pre>"},{"location":"#caching-and-performance","title":"Caching and Performance","text":"<p><code>daidai</code> implements intelligent caching to optimize performance:</p> <ul> <li>Artifacts are cached based on their configuration parameters</li> <li>File dependencies use a content-addressed store for efficient storage</li> <li>Memory usage is tracked (when pympler is installed, <code>pip install daidai[memory]</code>)</li> <li>Cache invalidation is handled automatically based on dependency changes</li> </ul> <p>This ensures your ML components load quickly while minimizing redundant computation and memory usage.</p>"},{"location":"#environment-configuration","title":"\ud83d\udd27 Environment Configuration","text":"<p><code>daidai</code> can be configured through environment variables:</p> <ul> <li><code>DAIDAI_CACHE_DIR</code>: Directory for persistent file cache</li> <li><code>DAIDAI_CACHE_DIR_TMP</code>: Directory for temporary file cache</li> <li><code>DAIDAI_DEFAULT_CACHE_STRATEGY</code>: Default strategy for file caching, so you don't have to specify it for each file</li> <li><code>DAIDAI_FORCE_DOWNLOAD</code>: Force download even if cached versions exist</li> <li><code>DAIDAI_LOG_LEVEL</code>: Logging verbosity level</li> </ul>"},{"location":"#adaptable-design","title":"\ud83e\uddf0 Adaptable Design","text":"<p><code>daidai</code> embraces an adaptable design philosophy that provides core functionality while allowing for extensive customization and extension. This approach enables you to integrate <code>daidai</code> into your existing ML infrastructure without forcing rigid patterns or workflows.</p>"},{"location":"#pure-functions-as-building-blocks","title":"Pure Functions as Building Blocks","text":"<p>At its core, <code>daidai</code> uses pure functions decorated with <code>@artifact</code> and <code>@predictor</code> rather than class hierarchies or complex abstractions:</p> <pre><code>@artifact\ndef embedding_model(model_path: Path) -&gt; Model:\n    return load_model(model_path)\n\n@predictor\ndef embed_text(text: str, model: Annotated[Model, embedding_model]) -&gt; np.ndarray:\n    return model.encode(text)\n</code></pre> <p>This functional approach provides several advantages:</p> <ol> <li>Composability: Functions can be easily composed together to create complex pipelines</li> <li>Testability: Pure functions with explicit dependencies are straightforward to test</li> <li>Transparency: The data flow between components is clear and traceable</li> <li>Interoperability: Functions work with any Python object, not just specialized classes</li> </ol>"},{"location":"#integration-with-external-systems","title":"Integration with External Systems","text":"<p>You can easily integrate <code>daidai</code> with external systems and frameworks:</p> <pre><code># Integration with existing ML experiment tracking\nimport mlflow\n@artifact\ndef tracked_model(model_id: str, mlflow_uri: str) -&gt; Model:\n    mlflow.set_tracking_uri(mlflow_uri)\n    model_uri = f\"models:/{model_id}/Production\"\n    return mlflow.sklearn.load_model(model_uri)\n\n# Integration with metrics collection\n@predictor\ndef classified_with_metrics(\n    text: str,\n    model: Annotated[Model, classifier_model],\n    metrics_client: Annotated[MetricsClient, metrics]\n) -&gt; str:\n    result = model.predict(text)\n    metrics_client.increment(\"prediction_count\")\n    metrics_client.histogram(\"prediction_latency\", time.time() - start_time)\n    return result\n</code></pre>"},{"location":"#adding-your-own-capabilities","title":"Adding Your Own Capabilities","text":"<p><code>daidai</code> can be extended with additional capabilities by composing with other libraries:</p>"},{"location":"#inputoutput-validation-with-pydantic","title":"Input/Output Validation with Pydantic","text":"<pre><code># Apply validation to predictor\n@predictor\n@validate_call(validate_return=True)\ndef analyze_sentiment(\n    text: TextInput,\n    model: Annotated[Model, sentiment_model],\n    min_length: int = 1\n) -&gt; SentimentResult:\n    # Input has been validated\n    result = model.predict(text)\n    return SentimentResult(\n        sentiment=result.label,\n        confidence=result.score,\n    ) # Output will be validated\n</code></pre>"},{"location":"#performance-optimization-with-lru-cache","title":"Performance Optimization with LRU Cache","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1000)\n@predictor\ndef classify_text(\n    text: str,\n    model: Annotated[Model, classifier_model]\n) -&gt; str:\n    # This result will be cached based on text if only text is provided (and model injected)\n    return model.predict(text)\n</code></pre>"},{"location":"#instrumentation-and-observability","title":"Instrumentation and Observability","text":"<pre><code>from opentelemetry import trace\nimport time\nfrom functools import wraps\n\ntracer = trace.get_tracer(__name__)\n\ndef traced_predictor(func):\n    \"\"\"Decorator to add tracing to predictors\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        with tracer.start_as_current_span(func.__name__):\n            return func(*args, **kwargs)\n    return wrapper\n\ndef timed_predictor(func):\n    \"\"\"Decorator to measure and log execution time\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        execution_time = time.perf_counter() - start_time\n        print(f\"{func.__name__} executed in {execution_time:.4f} seconds\")\n        return result\n    return wrapper\n\n@predictor\n@traced_predictor\n@timed_predictor\ndef predict_with_instrumentation(\n    text: str,\n    model: Annotated[Model, model]\n) -&gt; str:\n    # This call will be traced and timed\n    return model.predict(text)\n</code></pre>"},{"location":"#replacing-components","title":"Replacing Components","text":"<p>The dependency injection system allows you to replace components at runtime without modifying any code:</p> <pre><code># Normal usage with automatic dependency resolution\nresult = embed_text(\"Example text\")\n\n# Replace the embedding model for A/B testing\nexperimental_model = load_experimental_model()\nresult_b = embed_text(\"Example text\", model=experimental_model)\n\n# Replace for a specific use case\nsmall_model = load_small_model()\nbatch_results = [embed_text(t, model=small_model) for t in large_batch]\n</code></pre> <p><code>daidai</code>'s adaptable design ensures that you can build ML systems that meet your specific requirements while still benefiting from the core dependency management and caching features. Whether you're working on a simple prototype or a complex production system, <code>daidai</code> provides the flexibility to adapt to your needs without getting in your way.</p>"},{"location":"#concurrency-parallelism","title":"\ud83e\uddf5 Concurrency &amp; Parallelism","text":"<p>While file operations are protected against race conditions (downloading, caching etc.), other operations are not due to the lazy nature of component loading. As such, <code>daidai</code> cannot be considered thread-safe and does not plan to in the short term.</p> <p>However, there are ways to work around this limitation for multi-threaded applications:</p> <ol> <li>Create a shared <code>ModelManager</code> instance for all threads, but ensure that components are loaded before the threads are started:</li> </ol> <pre><code>@artifact\ndef model(model_path: Annotated[Path, \"s3://bucket/model.pkl\"]) -&gt; Model:\n    with open(model_path, \"rb\") as f:\n        return pickle.load(f)\n\n@predictor\ndef sentiment_classifier(text: str, model: Annotated[Model, model]):\n    return model.predict(text)\n\n\nwith ModelManager(preload=[sentiment_classifier]) as manager:\n    # sentiment_classifier and its dependencies (model) are loaded and\n    # ready to be used by all threads without issues\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        results = list(executor.map(worker_function, data_chunks))\n</code></pre> <ol> <li>Create a separate <code>ModelManager</code> instance for each thread, each will benefit from the same disk cache but will not share components:</li> </ol> <pre><code># same predictor &amp; artifact definitions as above\n\ndef worker_function(data_chunk):\n    # Each thread has its own manager and namespace\n    with ModelManager(namespace=str(threading.get_ident())) as manager:\n        return my_predictor(data)\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(worker_function, data_chunks))\n</code></pre> <p>A few notes:</p> <ul> <li>Creating separate ModelManager instances (approach #2) might lead to duplicate loading of the same components in memory across threads, while preloading (approach #1) ensures components are shared but requires knowing &amp; loading all components in advance.</li> <li>For most applications, approach #2 (separate managers) provides the safest experience, while approach #1 (preloading) is more memory-efficient and simple to implement for applications with large models.</li> <li>Both approaches benefit from disk caching, so model files are only downloaded once regardless of how many ModelManager instances you create.</li> </ul>"},{"location":"#testing","title":"\ud83e\uddea Testing","text":"<p><code>daidai</code>'s design makes testing ML components straightforward and effective. The dependency injection pattern allows for clean separation of concerns and easy mocking of dependencies.</p>"},{"location":"#unit-testing-components","title":"Unit Testing Components","text":"<p>When unit testing artifacts or predictors, you can manually inject dependencies:</p> <pre><code>def test_text_classifier():\n    # Create a mock model that always returns \"positive\"\n    mock_model = lambda text: \"positive\"\n\n    # Pass the mock directly instead of using the real model\n    result = classify_text(\"Great product!\", model=mock_model)\n\n    assert result == \"positive\"\n</code></pre>"},{"location":"#testing-with-fixtures","title":"Testing with Fixtures","text":"<p>In pytest, you can create fixtures that provide mock artifacts:</p> <pre><code>import pytest\n\n@pytest.fixture\ndef mock_embedding_model():\n    # Return a simplified embedding model for testing\n    return lambda text: np.ones(768) * 0.1\n\ndef test_semantic_search(mock_embedding_model):\n    # Use the fixture as a dependency\n    results = search_documents(\n        \"test query\",\n        embedding_model=mock_embedding_model\n    )\n    assert len(results) &gt; 0\n</code></pre>"},{"location":"#integration-testing","title":"Integration Testing","text":"<p>For integration tests that verify the entire component pipeline:</p> <pre><code>@pytest.fixture(scope=\"module\")\ndef test_model_manager():\n    # Set up a test namespace with real components\n    with ModelManager(\n        preload=[classify_text],\n        namespace=\"test\"\n    ) as manager:\n        yield manager\n\ndef test_end_to_end_classification(test_model_manager):\n    # This will use real components in the test namespace\n    result = classify_text(\"Test input\")\n    assert result in [\"positive\", \"negative\", \"neutral\"]\n</code></pre>"},{"location":"#testing-file-dependencies","title":"Testing File Dependencies","text":"<p>For file dependencies, you can use local test files:</p> <pre><code>@artifact\ndef test_embeddings(\n    embeddings_file: Annotated[\n        Path,\n        \"file:///path/to/test_embeddings.npy\"\n    ]\n) -&gt; np.ndarray:\n    return np.load(embeddings_file)\n\n# In your test\ndef test_with_test_embeddings():\n    result = embed_text(\"test\", embeddings=test_embeddings())\n    assert result.shape == (768,)\n</code></pre> <p><code>daidai</code>'s flexible design ensures that your ML components remain testable at all levels, from unit tests to integration tests, without requiring complex mocking frameworks or test setup.</p>"},{"location":"#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>daidai \ud83c\udf4a Documentation</li> <li>daidai \ud83c\udf4a GitHub Repository</li> <li>daidai \ud83c\udf4a PyPI Package</li> </ul>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"}]}